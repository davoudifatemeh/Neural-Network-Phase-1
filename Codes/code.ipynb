<div dir="rtl">
    <font size="5"><b>پروژه پنجم (فاز اول) – پیاده سازی و آموزش شبکه های عصبی Feed Forward</b><br></font>
    <font size="3">هدف پروژه: در فاز اول پروژه پنجم به پیاده سازی شبکه های عصبی چند لایه  جهت طبقه بندی تصاویر می پردازیم.<br>
        مجموعه داده: در این پروژه با یک مجموعه داده KMnist شامل تصاویری در مقیاس 28 در 28 پیکسل بصورت grayscale و 75000 تصویر از 20 کلاس مختلف کار خواهیم کرد.<br></font>
</div>
<div dir="rtl">
    <font size="4"><b>فاز اول: بررسی و پیش پردازش داده ها</b><br></font>
</div>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
train_images = np.genfromtxt('train_images.csv', delimiter=',')
train_images = np.delete(train_images, (0), axis=0)
train_images = np.delete(train_images, (0), axis=1)
train_labels = np.genfromtxt('train_labels.csv', delimiter=',')
train_labels = np.delete(train_labels, (0), axis=0)
train_labels = np.delete(train_labels, (0), axis=1)
test_images = np.genfromtxt('test_images.csv', delimiter=',')
test_images = np.delete(test_images, (0), axis=0)
test_images = np.delete(test_images, (0), axis=1)
test_labels = np.genfromtxt('test_labels.csv', delimiter=',')
test_labels = np.delete(test_labels, (0), axis=0)
test_labels = np.delete(test_labels, (0), axis=1)
train_labels = train_labels.reshape((60000, 1))
test_labels = test_labels.reshape((15000, 1))
<div dir="rtl">
    <font size="3">• بررسی یک تصویر در مجموعه داده train بصورت رندوم (مقدار هر پیکسل عددی بین صفر و 255 است):</font>
</div>
plt.figure()
plt.xlabel('Class ' + str(train_labels[1000][0].astype(np.uint8)))
plt.imshow(np.reshape(train_images[1000], (28, 28)), cmap='gray')
<div dir="rtl">
    <font size="3">• نمایش یک تصویر دلخواه و نوع آن از هر کلاس در مجموعه داده train</font>
</div>
for i in range(20):
    plt.figure()
    plt.xlabel('Class ' + str(i))
    plt.imshow(np.reshape(train_images[np.where(train_labels == i)[0][100]], (28, 28)), cmap='gray')
<div dir="rtl">
    <font size="3">• رسم نمودار میله ای فراوانی داده های هر کلاس در مجموعه داده های test و train</font>
</div>
x = [int(i) for i in range(20)]

y_train_labels = [0]*20
y_test_labels = [0]*20

for label in list(train_labels):
    y_train_labels[label[0].astype(np.uint8)] += 1
    
for label in test_labels:
    y_test_labels[label[0].astype(np.uint8)] += 1

fig, ax = plt.subplots()
ax.bar(x, y_train_labels, 0.6, label='Train', color='lightseagreen')
ax.bar(x, y_test_labels, 0.6, label='Test', color='gold')
ax.set_ylabel('Frequency')
ax.set_xlabel('Class Number')
ax.set_xticks(x)
ax.legend()
plt.show()
<div dir="rtl">
    <font size="3"><br>• نرمال کردن داده های test و train</font>
    <font size="3"><br>داده ها را نرمالایز می کنیم تا از سرریز کردن یا کوچک شدن بیش از اندازه گرادیان (NaN) جلوگیری کنیم. همچنین این کار موجب می شود داده ها دارای scale مشترک شوند (داده هایی با چندین ویژگی عددی متفاوت)</font>
</div>
train_images = train_images / 255
test_images = test_images / 255
<div dir="rtl">
    <font size="4"><b>فاز دوم: تکمیل بخش های ناقص شبکه عصبی</b><br></font>
</div>
# Dataloader
class Dataloader:
    
    def __init__(self, data, labels, n_classes, batch_size=None, shuffle=False):

        assert len(data)==len(labels)
        self.__n_classes = n_classes
        self.__batch_size = batch_size
        self.__shuffle = shuffle
        self.__data = data
        self.__onehot_labels = self.__onehot(labels, self.__n_classes)
    
    def __onehot(self, labels, n_classes):
        onehot_vectors = np.eye(n_classes)[labels.astype(np.uint8)]
        return onehot_vectors
    
    def __shuffle_dataset(self):
        randomize = np.arange(len(self.__data))
        np.random.shuffle(randomize)
        self.__data = self.__data[randomize]
        self.__onehot_labels = self.__onehot_labels[randomize]
    
    def __iter__(self):
        
        if self.__shuffle:
            self.__shuffle_dataset()
            
        if self.__batch_size==None:
            yield (np.matrix(self.__data), np.matrix(self.__onehot_labels))
            return
            
        for idx in range(0, len(self.__data), self.__batch_size):
            yield (np.matrix(self.__data[idx:idx+self.__batch_size]), 
                   np.matrix(self.__onehot_labels[idx:idx+self.__batch_size]))
# Activation Functions
class Identical:
    
    def __init__(self): pass
    
    def __val(self, matrix):
        identical_value = np.matrix(matrix, dtype=float)
        return identical_value

    def derivative(self, matrix):
        temp = np.matrix(matrix, dtype=float)
        identical_derivative = np.matrix(np.full(np.shape(temp), 1.))
        return identical_derivative
    
    def __call__(self, matrix):
        return self.__val(matrix)
    

class Relu:
    
    def __init__(self): pass
    
    def __val(self, matrix):
        relu_value = np.matrix(np.maximum(matrix, 0), dtype=float)
        return relu_value

    def derivative(self, matrix):
        relu_derivative = np.matrix(np.vectorize(lambda x: 1 if x >= 0 else 0)(matrix), dtype=float)
        return relu_derivative
    
    def __call__(self, matrix):
        return self.__val(matrix)

    
class LeakyRelu:
    
    def __init__(self, negative_slope=0.01):
        self.negative_slope = 0.01
    
    def __val(self, matrix):
        leacky_relu_value = np.matrix(np.vectorize(lambda x: x if x >= 0 else x * self.negative_slope)(matrix), dtype=float)
        return leacky_relu_value

    def derivative(self, matrix):
        leacky_relu_derivative = np.matrix(np.vectorize(lambda x: 1 if x >= 0 else self.negative_slope)(matrix), dtype=float)
        return leacky_relu_derivative
    
    def __call__(self, matrix):
        return self.__val(matrix)

    
class Sigmoid:
    
    def __init__(self): pass

    def __val(self, matrix):
        sigmoid_value = np.matrix(np.vectorize(lambda x: 1 / (1 + np.exp(-x)))(matrix), dtype=float)
        return sigmoid_value

    def derivative(self, matrix):
        sigmoid_derivative = np.matrix(np.vectorize(lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x)))))(matrix), dtype=float)
        return sigmoid_derivative
    
    def __call__(self, matrix):
        return self.__val(matrix)


class Softmax:
    
    def __init__(self): pass

    def __val(self, matrix):
        temp = np.matrix(matrix - np.max(matrix, axis=1), dtype=float)
        numerator = np.exp(temp)
        denominator = np.sum(numerator, axis=1)
        softmax_value = numerator / denominator
        return softmax_value
    
    def __call__(self, matrix):
        return self.__val(matrix)
    
class Tanh:
    
    def __init__(self): pass

    def __val(self, matrix):
        tanh_value = np.matrix(np.vectorize(lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))(matrix), dtype=float)
        return tanh_value

    def derivative(self, matrix):
        tanh_derivative = np.matrix(np.vectorize(lambda x: (1 - np.power((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)), 2)))(matrix), dtype=float)
        return tanh_derivative
    
    def __call__(self, matrix):
        return self.__val(matrix)
# Loss Function
class CrossEntropy: #(with softmax)
    
    def __init__(self): pass

    def __val(self, true_val, expected_val):
        assert np.shape(true_val)==np.shape(expected_val)
        temp = np.matrix(true_val - np.max(true_val, axis=1), dtype=float)
        numerator = np.exp(temp)
        denominator = np.sum(numerator, axis=1)  
        softmax_value = numerator / denominator
        softmax_value = np.clip(softmax_value, 1e-12, 1. - 1e-12)
        N = softmax_value.shape[0]
        cross_entropy_value = -np.sum(np.multiply(np.matrix(expected_val, dtype=float), np.log(softmax_value + 1e-9))) / N
        return cross_entropy_value
        
    def derivative(self, true_val, expected_val):
        assert np.shape(true_val)==np.shape(expected_val)
        temp = np.matrix(true_val - np.max(true_val, axis=1), dtype=float)
        numerator = np.exp(temp)
        denominator = np.sum(numerator, axis=1)
        softmax_value = numerator / denominator
        cross_entropy_derivative = softmax_value - np.matrix(expected_val, dtype=float)
        return cross_entropy_derivative
    
    def __call__(self, true_val, expected_val):
        return self.__val(true_val, expected_val)
<div dir="rtl">
    <font size="3"><br>
برای آپدیت وزن ها از رابطه مطرح شده در کلاس و جلسه توجیهی استفاده می شود. برای آپدیت بایاس کافی است فرض کنیم هر داده ی ورودی یک ویژگی با مقدار همواره 1 دارد. در این صورت می توان آپدیت بایاس را مشابه آپدیت وزن ها در نظر گرفت.</font>
</div>
# Layer
class Layer:

    DEFAULT_LOW, DEFAULT_HIGH, DEFAULT_MEAN, DEFAULT_VAR = 0, 0.05, 0., 1.
  
    def __init__(self, input_size, output_size, 
                 activation=Identical(), initial_weight='uniform', **initializing_parameters):
        
        assert type(initial_weight)==str, 'Undefined activation function!'
        
        self.__weight_initializer_dict = {'uniform':self.__uniform_weight, 'normal':self.__normal_weight}
        
        assert initial_weight in self.__weight_initializer_dict, 'Undefined weight initialization function!'


        self.__n_neurons = output_size
        weight_initializer = self.__weight_initializer_dict[initial_weight]
        self.__weight = weight_initializer(input_size, self.__n_neurons, **initializing_parameters)
        self.__bias = weight_initializer(1, self.__n_neurons, **initializing_parameters)
        self.__activation = activation
        
        self.__last_input = None
        self.__last_activation_input = None
        self.__last_activation_output = None
        self.__last_activation_derivative = None
        
    def forward(self, layer_input):
        assert np.ndim(layer_input)==2
        assert np.size(self.__weight,0) == np.size(layer_input,1)
        self.__last_input = np.matrix(layer_input, dtype=float)
        self.__last_activation_input = self.__last_input * self.__weight + self.__bias
        self.__last_activation_derivative = self.__activation.derivative(self.__last_activation_input)
        self.__last_activation_output = self.__activation(self.__last_activation_input)
        return self.__last_activation_output
    
    def update_weights(self, backprop_tensor, lr):
        assert np.ndim(backprop_tensor)==2
        assert np.size(backprop_tensor,0) == np.size(self.__last_activation_derivative,0)
        assert np.size(backprop_tensor,1) == self.__n_neurons
        backprop_tensor = np.matrix(backprop_tensor, dtype=float)
        delta_weight = np.matrix.transpose(self.__last_input) * np.multiply(backprop_tensor, self.__last_activation_derivative)
        temp = np.matrix(np.ones((np.size(backprop_tensor, 0), 1)), dtype=float)
        delta_bias = np.matrix.transpose(temp) * np.multiply(backprop_tensor, self.__last_activation_derivative)
        backprop_tensor = np.multiply(backprop_tensor, self.__last_activation_derivative) * np.matrix.transpose(self.__weight)
        self.__bias = self.__bias - (lr * delta_bias)
        self.__weight = self.__weight - (lr * delta_weight)
        return backprop_tensor

    def __uniform_weight(self, dim1, dim2, **initializing_parameters):
        low, high = self.DEFAULT_LOW, self.DEFAULT_HIGH
        if 'low' in initializing_parameters.keys(): low = initializing_parameters['low']
        if 'high' in initializing_parameters.keys(): high = initializing_parameters['high']
        weights = np.matrix(np.random.uniform(low, high, (dim1, dim2)), dtype=float)
        return weights

    def __normal_weight(self, dim1, dim2, **initializing_parameters):
        mean, var = self.DEFAULT_MEAN, self.DEFAULT_VAR
        if 'mean' in initializing_parameters.keys(): mean = initializing_parameters['mean']
        if 'var' in initializing_parameters.keys(): var = initializing_parameters['var']
        weights = np.matrix(np.random.normal(mean, np.sqrt(var), (dim1, dim2)), dtype=float)
        return weights
    
    @property
    def n_neurons(self): return self.__n_neurons
    
    @property
    def weight(self): return self.__weight
    
    @property
    def bias(self): return self.__bias
    
    @property
    def activation(self): return self.__activation
# Feed Forward Neural Network
class FeedForwardNN:
    
    def __init__(self, input_shape):
        
        self.__input_shape = input_shape
        self.__output_shape = None
        
        self.__layers_list = []
        
        self.__lr = None
        self.__loss = None

        
    def add_layer(self, n_neurons, activation=Relu(), initial_weight='uniform', **initializing_parameters):
         
        assert type(n_neurons)==int, "Invalid number of neurons for the layer!"
        assert n_neurons>0, "Invalid number of neurons for the layer!"
        
        n_prev_neurons = self.__input_shape if len(self.__layers_list)==0 else self.__layers_list[-1].n_neurons
        new_layer = Layer(n_prev_neurons, n_neurons, activation, initial_weight, **initializing_parameters)
        self.__layers_list.append(new_layer)
        self.__output_shape = self.__layers_list[-1].n_neurons 
      
    
    def set_training_param(self, loss=CrossEntropy(), lr=1e-3):
        assert self.__layers_list, "Uncomplete model!"
        self.__loss = loss
        self.__lr = lr
    
    
    def forward(self, network_input):
        assert type(self.__output_shape) != None, "Model is not compiled!"
        network_output = network_input
        for i in range(len(self.__layers_list)):
            network_output = self.__layers_list[i].forward(network_output)
        return network_output
    
    
    def fit(self, epochs, trainloader, testloader=None, print_results=True):
        
        assert type(self.__output_shape) != None, "Model is not compiled!"
        assert type(self.__lr) != None and type(self.__loss) != None, "Training paramenters are not set!"

        log = {"train_accuracy":[], "train_loss":[], "test_accuracy":[], "test_loss":[]}
        
        for epoch in range(1, epochs+1):
            
            if print_results: 
                print('Epoch {}:'.format(epoch)) 
                
            average_accuracy, average_loss = self.__train(trainloader)
            log['train_accuracy'].append(average_accuracy)
            log['train_loss'].append(average_loss)
            if print_results:
                print('\tTrain: Average Accuracy: {}\tAverage Loss: {}'.format(average_accuracy, average_loss))
            
            if type(testloader) != type(None):
                average_accuracy, average_loss = self.__test(testloader)
                log['test_accuracy'].append(average_accuracy)
                log['test_loss'].append(average_loss)
                if print_results:
                    print('\tTest: Average Accuracy: {}\tAverage Loss: {}'.format(average_accuracy, average_loss))
                    
        return log
    
    
    def __train(self, trainloader):
        bach_accuracies, batch_losses = [], []
        for x_train, y_train in trainloader:
            batch_accuracy, batch_loss = self.__train_on_batch(x_train, y_train)
            bach_accuracies.append(batch_accuracy)
            batch_losses.append(batch_loss)
        return np.mean(bach_accuracies), np.mean(batch_losses)
    
    
    def __test(self, testloader):
        bach_accuracies, batch_losses = [], []
        for x_test, y_test in testloader:
            batch_accuracy, batch_loss = self.__test_on_batch(x_test, y_test)
            bach_accuracies.append(batch_accuracy)
            batch_losses.append(batch_loss)
        return np.mean(bach_accuracies), np.mean(batch_losses)

    
    def __train_on_batch(self, x_batch, y_batch):
        output = self.forward(x_batch)
        batch_accuracy = self.__compute_accuracy(output, y_batch)
        batch_average_loss = self.__loss(output, y_batch)
        self.__update_weights(output, y_batch)
        return (batch_accuracy, batch_average_loss)
        
        
    def __test_on_batch(self, x_batch, y_batch):
        output = self.forward(x_batch)
        batch_accuracy = self.__compute_accuracy(output, y_batch)
        batch_average_loss = self.__loss(output, y_batch)
        return (batch_accuracy, batch_average_loss)
            
        
    def __get_labels(self, outputs):
        labels = outputs.argmax(axis=1)
        return labels
    
    
    def __compute_accuracy(self, output, expected_output):
        labels = self.__get_labels(output)
        expected_labels = self.__get_labels(expected_output)
        temp = labels - expected_labels
        accuracy = np.matrix(np.vectorize(lambda x: 1 if x == 0 else 0)(temp), dtype=float)
        return np.mean(accuracy)
    
    
    def __update_weights(self, output, y_train):
        backprop_tensor = self.__loss.derivative(output, y_train)
        for i in range(len(self.__layers_list) - 1, -1, -1):
            backprop_tensor = self.__layers_list[i].update_weights(backprop_tensor, self.__lr)
        return
<div dir="rtl">
    <font size="4"><b>فاز سوم: طبقه بندی داده ها</b><br></font>
    <font size="3">قسمت اول) آموزش شبکه</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">قسمت دوم) وزن دهی شبکه<br>
    درصورت صفر قرار دادن وزنها، مقدار Average Loss حدودا 2.9 باقی میماند و Average Accuracy نیز مقادیر 0.056 تا 0.057 خواهد داشت.</font>
</div>
<div dir="rtl">
    <font size="3">قسمت سوم) تاثیر learning rate<br>
    افزایش بیش از حد این متغییر ممکن است باعث سرریز شدن گرادیان و نرسیدن به global minimum شود. کاهش بیش از حد آن نیز میتواند باعث کند شدن فرآیند آموزش (دیرتر رسیدن به global minimum) شود.</font>
</div>
<div dir="rtl">
    <font size="3">ده برابر کردن learning rate:</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.01
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">0.1 برابر کردن learning rate:</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.0001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">قسمت چهارم) تاثیر activation function <br>
    از آنجایی که در توابع Sigmoid و Tanh محاسبات exp وجود دارد این توابع کندتر هستند. همچنین در تابع sigmoid ممکن است که گرادیان صفر شود <br>
    تابع LeakyRelu از آن جهت نسبت به relu برتری دارد که در این تابع مقادیر منفی نادیده گرفته نمیشوند و هیچ از یک از ویژگی ها از دست نمیرود.</font>
</div>
<div dir="rtl">
    <font size="3">تابع فعالساز LeakyRelu</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=LeakyRelu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">تابع فعالساز sigmoid</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Sigmoid(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">تابع فعالساز Tanh</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Tanh(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">قسمت پنجم) تاثیر batch size<br>
    استفاده از Batch Size موجب می شود که شبکه در هر ایپاک، به جای مشاهده ی کامل تمامی داده ها و آپدیت وزن ها، داده ها را به صورت دسته ای مشاهده کند و پس از مشاهده ی هر دسته وزن ها را آپدیت کند.<br>
        هرچه Batch Size بزرگتر شود، مدت زمان هر ایپاک کمتر خواهد شد و در تعداد ایپاک های بیشتری به مدل مطلوب خواهیم رسید
    در عوض هرچه Batch Size کوچکتر شود، مدت زمان هر ایپاک بیشتر خواهد شد و از طرف دیگر در تعداد ایپاک های کمتری به مدل مطلوب خواهیم رسید.<br>
    <br>
    استفاده از Batch Size های خیلی کوچک از یک طرف زمان هر ایپاک را بطور چشمگیری افزایش میدهد و از طرف دیگر آموزش مدل را مختل می کند. زیرا شبکه در هر Batch، داده های کاملا جدیدی خواهد دید و وزن های خود را با توجه به آن Batch بسیار تغییر خواهد داد  و در Batch های بعدی نیز این روند معیوب ادامه خواهد یافت. علت این اتفاق این است که مقدار Batch Size باید به گونه ای باشد که داده های هر Batch، نماینده ی نسبتا مناسبی از تمامی داده ها باشند.</font>
</div>
<div dir="rtl">
    <font size="3">batch size = 16</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=16, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=16, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">batch size = 32</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=32, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=32, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">batch size = 256</font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 20
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=256, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=256, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
<div dir="rtl">
    <font size="3">قسمت ششم) تاثیر epoch<br>
        دلیل استفاده از چندین ایپاک، بهبود عملکرد شبکه است زیرا شبکه در هر ایپاک وزنهای خود به مقادیری بهتری آپدیت میکند.<br>
        افزایش بی رویه تعداد ایپاک ها میتواند باعث overfitting (یادگیری بیش از حد از داده های آموزش) شود و به این ترتیب شبکه برخلاف عملکرد مناسب برروی داده های آموزش، برروی داده های تست بخوبی کار نخواهد کرد.
    </font>
</div>
train_images_copy = np.copy(train_images)
train_labels_copy = np.copy(train_labels)
test_images_copy = np.copy(test_images)
test_labels_copy = np.copy(test_labels)
INPUT_SHAPE = 784
LEARNING_RATE = 0.001
EPOCHS = 200
TRAINLOADER = Dataloader(data=train_images_copy, labels=train_labels_copy, n_classes=20, batch_size=64, shuffle=True)
TESTLOADER = Dataloader(data=test_images_copy, labels=test_labels_copy, n_classes=20, batch_size=64, shuffle=True)


network = FeedForwardNN(INPUT_SHAPE)
network.add_layer(25, activation=Relu(), weight_initializer='uniform')
network.add_layer(20, activation=Identical(), weight_initializer='uniform')
network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)

log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)
epochs = range(1, 201)
plt.plot(epochs, log['train_accuracy'], color='yellowgreen', label='train_accuracy')
plt.plot(epochs, log['test_accuracy'], color='crimson', label='test_accuracy')
plt.plot(epochs, log['train_loss'], color='dodgerblue', label='train_loss')
plt.plot(epochs, log['test_loss'], color='gold', label='test_loss')
plt.gcf().set_size_inches((20, 10))
plt.legend();
<div dir="rtl">
    <font size="3">همانطور که مشاهده میشود که از یک مرحله به بعد loss در داده های تست روند صعودی پیدا میکند در حالیکه در داده های آموزش همان روند نزولی قبل (با شدت کمتر) را دارد.</font>
</div>
